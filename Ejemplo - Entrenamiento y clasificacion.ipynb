{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504795c7-af37-4d93-b8a9-86fd182b7fa7",
   "metadata": {},
   "source": [
    "# 1. All bands -  all channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5398709-49dc-4177-a940-de0e50253742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Cargar el archivo de características\n",
    "features_df = pd.read_csv(\"/home/andres_marin/Notebooks/Tesis/New features/all channels - all bands/New_alldata_allch_allbands.csv\", header=None)\n",
    "features = features_df.values\n",
    "\n",
    "# Shape of the data:\n",
    "print(\"Dimensions of the data:\", features.shape)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/home/andres_marin/Notebooks/Tesis/labels/Labels_All_Data.csv\",\n",
    "                     header=None)\n",
    "labels = df.iloc[:, 0].values\n",
    "\n",
    "# Verificar la forma y la distribución de las etiquetas\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"Firts values:\", labels[:10])  # Muestra las primeras 10 etiquetas\n",
    "\n",
    "# Verificar la distribución de las etiquetas\n",
    "label_distribution = Counter(labels)\n",
    "print(\"Labels dsitribution:\", label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b1a2f-dfe0-4aa9-a285-bcb9ca519e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shirt and sweater data:\n",
    "dftrainSS = pd.read_csv(\"/home/andres_marin/Notebooks/Tesis/labels/Labels_Shirt_Sweater.csv\",\n",
    "                     header=None)\n",
    "labelsSS = dftrainSS.iloc[:, 0].values\n",
    "\n",
    "# Cargar el archivo de características (suponiendo que las características están en un formato adecuado)\n",
    "featurestrainSS = pd.read_csv(\"/home/andres_marin/Notebooks/Tesis/New features/all channels - all bands/New_both_allch_allbands.csv\", header=None)\n",
    "featuresSS = featurestrainSS.values\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "x_div1, x_div2, y_div1, y_div2 = train_test_split(\n",
    "    featuresSS, labelsSS, test_size=0.50, stratify=labelsSS, random_state=42)\n",
    "\n",
    "features = np.vstack((features, x_div1))\n",
    "\n",
    "# Imprimiendo las dimensiones para verificar\n",
    "print(\"Dimensiones de la matriz de características combinada:\", features.shape)\n",
    "\n",
    "# Cargando y combinando los labels de manera similar si es necesario\n",
    "labels = np.concatenate((labels, y_div1))\n",
    "\n",
    "# Verificación de las dimensiones\n",
    "print(\"Dimensiones de los labels combinados:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841afbcb-bc0f-44af-a32c-b67151ca0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.3, stratify=labels, random_state=42)\n",
    "\n",
    "# Verificar la distribución de las clases en los conjuntos de entrenamiento y validación\n",
    "train_counts = Counter(y_train)\n",
    "val_counts = Counter(y_test)\n",
    "\n",
    "print(\"\\nDistribución de clases en el conjunto de entrenamiento:\", train_counts)\n",
    "print(\"Distribución de clases en el conjunto de validación:\", val_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c8f53a-e8b4-4754-9556-e505d670f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Guardar el RobustScaler\n",
    "scaler=load('/home/andres_marin/Notebooks/Tesis/Finals_tests/All bands - all channels/Selectors/scaler.joblib')\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# Verificar la normalización con RobustScaler\n",
    "print(f'Train Min: {x_train.min()}')\n",
    "print(f'Train Max: {x_train.max()}')\n",
    "print(f'Test Min: {x_test.min()}')\n",
    "print(f'Test Max: {x_test.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f6aa7-21d1-42d5-ab2b-3ce89c9a9803",
   "metadata": {},
   "source": [
    "## Selectors features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822cc28a-dc0b-4ade-969b-8abcbd10aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Generar una lista de números de características (por ejemplo, del 1 al 280)\n",
    "numeros_caracteristicas = list(range(1, 281))\n",
    "\n",
    "# Obtener las características seleccionadas (como una lista de números)\n",
    "caracteristicas_seleccionadas = [numero for numero, seleccionado in zip(numeros_caracteristicas, RFE_svm.support_) if seleccionado]\n",
    "\n",
    "# Para RFE\n",
    "caracteristicas_rfe = [numeros_caracteristicas[i] for i in RFE_svm.get_support(indices=True)]\n",
    "\n",
    "print(\"Características seleccionadas:\", caracteristicas_rfe)\n",
    "#print(\"Características eliminadas:\", caracteristicas_eliminadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d16f5-766c-4484-9c4b-fcc498a76063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los índices de las características seleccionadas\n",
    "indices_seleccionados = sfs_lda.k_feature_idx_\n",
    "\n",
    "# Para SelectFromModel (L1, RandomForest, etc.)\n",
    "caracteristicas_sfs = [numeros_caracteristicas[i] for i in sfs_lda.k_feature_idx_]\n",
    "\n",
    "print(\"Características seleccionadas:\", caracteristicas_sfs)\n",
    "#print(\"Características eliminadas:\", caracteristicas_eliminadas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9a1c4-5b93-425d-9cac-27c6e5e7308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar una lista de números de características (por ejemplo, del 1 al 280)\n",
    "numeros_caracteristicas = list(range(1, 281))\n",
    "\n",
    "# Obtener las características seleccionadas (como una lista de números)\n",
    "caracteristicas_seleccionadas = [numero for numero, seleccionado in zip(numeros_caracteristicas, sfm.get_support()) if seleccionado]\n",
    "caracteristicas_sfm = [numeros_caracteristicas[i] for i in sfm.get_support(indices=True)]\n",
    "\n",
    "print(\"Características seleccionadas:\", caracteristicas_sfm)\n",
    "#print(\"Características eliminadas:\", caracteristicas_eliminadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a38d0d-8c0f-44d3-b631-e7de56ee31c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar una lista de números de características (por ejemplo, del 1 al 280)\n",
    "numeros_caracteristicas = list(range(1, 281))\n",
    "\n",
    "# Obtener los índices de las características seleccionadas por el selector óptimo\n",
    "indices_seleccionados = ms_knn.get_support(indices=True)\n",
    "\n",
    "# Convertir los índices seleccionados a números de características\n",
    "caracteristicas_seleccionadas = [numeros_caracteristicas[i] for i in indices_seleccionados]\n",
    "\n",
    "# Para mutual information (SelectKBest)\n",
    "caracteristicas_mi = [numeros_caracteristicas[i] for i in ms_knn.get_support(indices=True)]\n",
    "\n",
    "print(\"Características seleccionadas:\", caracteristicas_mi)\n",
    "#print(\"Características eliminadas:\", caracteristicas_eliminadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a9f95-1704-446c-975e-054023020e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Generar una lista de números de características (por ejemplo, del 1 al 280)\n",
    "numeros_caracteristicas = list(range(1, 281))\n",
    "\n",
    "# Obtener los índices de las características seleccionadas\n",
    "indices_seleccionados = l1.get_support(indices=True)\n",
    "\n",
    "# Para L1-based feature selection\n",
    "caracteristicas_l1 = [numeros_caracteristicas[i] for i in l1.get_support(indices=True)]\n",
    "\n",
    "print(\"Características seleccionadas:\", caracteristicas_l1)\n",
    "#print(\"Características eliminadas:\", caracteristicas_eliminadas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7d885-17bd-42ab-b6df-4bcd95c2032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rfe = set(caracteristicas_rfe)\n",
    "set_sfs = set(caracteristicas_sfs)\n",
    "set_sfm = set(caracteristicas_sfm)\n",
    "set_mi = set(caracteristicas_mi)\n",
    "set_l1 = set(caracteristicas_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92543742-7d42-45b8-9ba4-6ac8951076c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Inicializar el diccionario para contar apariciones\n",
    "conteo_caracteristicas = defaultdict(int)\n",
    "\n",
    "# Contar las apariciones en cada selector\n",
    "for caracteristica in caracteristicas_rfe:\n",
    "    conteo_caracteristicas[caracteristica] += 1\n",
    "\n",
    "for caracteristica in caracteristicas_sfs:\n",
    "    conteo_caracteristicas[caracteristica] += 1\n",
    "\n",
    "for caracteristica in caracteristicas_sfm:\n",
    "    conteo_caracteristicas[caracteristica] += 1\n",
    "\n",
    "for caracteristica in caracteristicas_mi:\n",
    "    conteo_caracteristicas[caracteristica] += 1\n",
    "\n",
    "for caracteristica in caracteristicas_l1:\n",
    "    conteo_caracteristicas[caracteristica] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738bb4d-b632-4aa9-8c92-d241f5f6ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer el umbral para la mayoría (por ejemplo, 3 de 5 selectores)\n",
    "umbral_mayoria = 2\n",
    "\n",
    "# Filtrar las características que cumplen con el umbral\n",
    "caracteristicas_por_mayoria = [caracteristica for caracteristica, conteo in conteo_caracteristicas.items() if conteo >= umbral_mayoria]\n",
    "\n",
    "print(\"Características seleccionadas por la mayoría:\", caracteristicas_por_mayoria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915c803-74aa-4731-8be8-1bcb1142a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Suponiendo que tienes 280 características\n",
    "total_caracteristicas = 280\n",
    "\n",
    "# Inicializar una máscara de selección con False para todas las características\n",
    "mascara_seleccion = np.zeros(total_caracteristicas, dtype=bool)\n",
    "\n",
    "# Marcar como True las características seleccionadas por la mayoría\n",
    "for caracteristica in caracteristicas_por_mayoria:\n",
    "    mascara_seleccion[caracteristica - 1] = True  # Restar 1 si tus características están numeradas desde 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434122a0-1886-4cab-aced-0dd58113512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class CustomFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mask):\n",
    "        self.mask = mask\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Filtrar las características usando la máscara\n",
    "        return X[:, self.mask]\n",
    "\n",
    "# Crear una máscara basada en las características seleccionadas por la mayoría\n",
    "mascara_seleccion = np.zeros(280, dtype=bool)  # Suponiendo 280 características\n",
    "\n",
    "for caracteristica in caracteristicas_por_mayoria:\n",
    "    mascara_seleccion[caracteristica - 1] = True  # Ajustar si las características comienzan en 1\n",
    "\n",
    "# Crear una instancia de tu selector personalizado\n",
    "custom_selector = CustomFeatureSelector(mask=mascara_seleccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee779c61-ea06-429b-9db6-6153c3395aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que X es tu conjunto de datos original\n",
    "X_nuevo = custom_selector.transform(x_train)\n",
    "\n",
    "print(\"Forma del conjunto de datos transformado:\", X_nuevo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f10e32-f8c5-433b-b20f-683ceccc8a0c",
   "metadata": {},
   "source": [
    "## Loading selectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de163e-51d5-4754-9f32-72de7c313a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# For SVM:\n",
    "sfs_lda=load('/home/andres_marin/Notebooks/Tesis/Finals_tests/All bands - all channels/Selectors/sfsLDA.joblib')\n",
    "\n",
    "# For LDA:\n",
    "sfm=load('/home/andres_marin/Notebooks/Tesis/Finals_tests/All bands - all channels/Selectors/sfmMean.joblib')\n",
    "ms_knn=load('/home/andres_marin/Notebooks/Tesis/Finals_tests/All bands - all channels/Selectors/MS_knn.joblib')\n",
    "\n",
    "# For KNN:\n",
    "RFE_svm=load('/home/andres_marin/Notebooks/Tesis/Finals_tests/All bands - all channels/Selectors/selector_rfecv_svm.joblib')\n",
    "\n",
    "# For RF:\n",
    "pca=load('/home/andres_marin/Notebooks/Tesis/Finals_tests/All bands - all channels/Selectors/pca.joblib')\n",
    "\n",
    "# For MLP:\n",
    "sfs_lda=load('/home/andres_marin/Notebooks/Tesis/Finals_tests/All bands - all channels/Selectors/sfsLDA.joblib')\n",
    "\n",
    "# L1\n",
    "l1=load('/home/andres_marin/Notebooks/Tesis/Finals_tests/All bands - all channels/Selectors/L1_Selector.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ebd94-7116-4304-9fd1-4841b3766a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sfslda=sfs_lda.transform(x_train)\n",
    "x_test_sfslda=sfs_lda.transform(x_test)\n",
    "x_test2_sfslda=sfs_lda.transform(x_test2)\n",
    "x_testSS_sfslda=sfs_lda.transform(x_div2)\n",
    "\n",
    "x_train_sfm=sfm.transform(x_train)\n",
    "x_test_sfm=sfm.transform(x_test)\n",
    "x_test2_sfm=sfm.transform(x_test2)\n",
    "x_testSS_sfm=sfm.transform(x_div2)\n",
    "\n",
    "x_train_msknn=ms_knn.transform(x_train)\n",
    "x_test_msknn=ms_knn.transform(x_test)\n",
    "x_test2_msknn=ms_knn.transform(x_test2)\n",
    "x_testSS_msknn=ms_knn.transform(x_div2)\n",
    "\n",
    "x_train_rfesvm=RFE_svm.transform(x_train)\n",
    "x_test_rfesvm=RFE_svm.transform(x_test)\n",
    "x_test2_rfesvm=RFE_svm.transform(x_test2)\n",
    "x_testSS_rfesvm=RFE_svm.transform(x_div2)\n",
    "\n",
    "x_train_pca=pca.transform(x_train)\n",
    "x_test_pca=pca.transform(x_test)\n",
    "x_test2_pca=pca.transform(x_test2)\n",
    "x_testSS_pca=pca.transform(x_div2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb770f7-aff4-4f7b-845d-1d7bdf0b3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sfs: {x_train_sfslda.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"sfm: {x_test_sfm.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"sfm: {x_test_msknn.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"sfm: {x_test_rfesvm.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"sfm: {x_test_pca.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c47fb4-f66c-4213-a3df-e91eed3e8d3a",
   "metadata": {},
   "source": [
    "# Parameters optimization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb1e183-5afd-4f22-a649-39c5ed2dd800",
   "metadata": {},
   "source": [
    "## SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4f53e-e650-4e7c-9473-b273eef52a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def objective(trial):\n",
    "    # Sugerencias para los hiperparámetros\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "    C = trial.suggest_float('C', 0.001, 10, log=True)\n",
    "\n",
    "    if kernel in ['poly', 'rbf', 'sigmoid']:\n",
    "        gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    else:\n",
    "        gamma = 'scale'  # Valor predeterminado para el kernel lineal\n",
    "\n",
    "    if kernel in ['poly', 'sigmoid']:\n",
    "        coef0 = trial.suggest_float('coef0', 0, 10)\n",
    "    else:\n",
    "        coef0 = 0.0  # Valor predeterminado para kernels que no utilizan coef0\n",
    "\n",
    "    if kernel == 'poly':\n",
    "        degree = trial.suggest_int('degree', 1, 4)\n",
    "    else:\n",
    "        degree = 3  # Valor predeterminado para otros kernels\n",
    "\n",
    "    # Crear el objeto del clasificador SVC con los hiperparámetros sugeridos\n",
    "    classifier_obj = SVC(kernel=kernel, C=C, gamma=gamma, degree=degree, coef0=coef0, random_state=42)\n",
    "\n",
    "    # Entrenar el modelo con el conjunto de entrenamiento\n",
    "    classifier_obj.fit(x_train_sfslda, y_train)\n",
    "\n",
    "    # Evaluar el modelo en el primer conjunto de prueba\n",
    "    y_pred = classifier_obj.predict(x_test2_sfslda)\n",
    "    accuracy = accuracy_score(y_test2, y_pred)\n",
    "\n",
    "    # Retornar el promedio de las dos precisiones\n",
    "    return accuracy\n",
    "\n",
    "# Crear un estudio de Optuna y optimizar\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=300)\n",
    "\n",
    "print('Mejores parámetros:', study.best_params)\n",
    "print('Mejor resultado de precisión:', study.best_value)\n",
    "\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "# Ordenar los ensayos por su valor (mejor a peor) y tomar los primeros 10\n",
    "best_trials = sorted(study.trials, key=lambda trial: trial.value, reverse=True)[:10]\n",
    "\n",
    "print(\"\\nTop 10 Trials:\")\n",
    "for trial in best_trials:\n",
    "    print(f\"Trial {trial.number} | Value: {trial.value} | Params: {trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5d641-b809-461b-8263-91955c9bc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Configuración del modelo con los mejores parámetros encontrados\n",
    "svm = SVC(kernel='poly',\n",
    "               C=8.869623761792718,\n",
    "               gamma='scale',\n",
    "               coef0=6.276384199891294,\n",
    "               degree=3,\n",
    "               random_state=42)\n",
    "\n",
    "# Train model:\n",
    "svm.fit(x_train_sfslda, y_train)\n",
    "\n",
    "# Pred with train data:\n",
    "y_pred = svm.predict(x_train_sfslda)\n",
    "\n",
    "# Pred with test data:\n",
    "y_pred2 = svm.predict(x_test_sfslda)\n",
    "\n",
    "# Pred with shirt & sweater data:\n",
    "y_pred3 = svm.predict(x_testSS_sfslda)\n",
    "\n",
    "# Calcular la precisión\n",
    "accuracy1 = accuracy_score(y_train, y_pred)\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "accuracy3 = accuracy_score(y_div2, y_pred3)\n",
    "\n",
    "print(\"Precisión del modelo en el conjunto de train: {:.2f}%\".format(accuracy1 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba: {:.2f}%\".format(accuracy2 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba de camisas y sweaters: {:.2f}%\".format(accuracy3 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5cdadc-2447-4e46-8aff-1007965574d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred2)\n",
    "\n",
    "# Graficar la matriz de confusión usando seaborn\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Verdaderos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b0f97-a8ca-4819-a4b9-d0292bb0d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la matriz de confusión\n",
    "conf_matrix2 = confusion_matrix(y_div2, y_pred3)\n",
    "\n",
    "# Graficar la matriz de confusión usando seaborn\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix2, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Verdaderos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61dbee0-8a1d-4d24-b585-6181884d8c80",
   "metadata": {},
   "source": [
    "## LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b1cfa-75c7-4d28-8cd6-99c01b85cb95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Supongamos que x_train, y_train, x_test, y_test están definidos\n",
    "\n",
    "def objective(trial):\n",
    "    # Sugerencias para los hiperparámetros\n",
    "    solver = trial.suggest_categorical('solver', ['svd', 'lsqr', 'eigen'])\n",
    "    shrinkage = None\n",
    "    if solver in ['lsqr', 'eigen']:\n",
    "        shrinkage = trial.suggest_categorical('shrinkage', ['auto', None])\n",
    "        \n",
    "    # Los priors pueden ser None o un arreglo que sume 1 y represente la probabilidad a priori de cada clase\n",
    "    num_classes = np.unique(y_train).size\n",
    "    if trial.suggest_categorical('optimize_priors', [True, False]):\n",
    "        priors = np.array([trial.suggest_float(f'prior_{i}', 0.01, 1) for i in range(num_classes)])\n",
    "        priors = priors / np.sum(priors)  # Normalizar para que sumen 1\n",
    "    else:\n",
    "        priors = None\n",
    "\n",
    "    # Crear el objeto del clasificador LDA con los hiperparámetros sugeridos\n",
    "    classifier_obj = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage, priors=priors)\n",
    "\n",
    "    # Entrenar el modelo con el conjunto de entrenamiento\n",
    "    classifier_obj.fit(x_train_sfm, y_train)\n",
    "\n",
    "    # Evaluar el modelo con el conjunto de prueba\n",
    "    y_pred = classifier_obj.predict(x_test2_sfm)\n",
    "    accuracy = accuracy_score(y_test2, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Crear un estudio de Optuna y optimizar\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=300)\n",
    "\n",
    "print('Mejores parámetros:', study.best_params)\n",
    "print('Mejor resultado de precisión:', study.best_value)\n",
    "\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "# Ordenar los ensayos por su valor (mejor a peor) y tomar los primeros 10\n",
    "best_trials = sorted(study.trials, key=lambda trial: trial.value, reverse=True)[:10]\n",
    "\n",
    "print(\"\\nTop 10 Trials:\")\n",
    "for trial in best_trials:\n",
    "    print(f\"Trial {trial.number} | Value: {trial.value} | Params: {trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d285d-3436-4b37-8747-4ca2c19fe938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parámetros obtenidos de Optuna\n",
    "solver = 'svd'\n",
    "shrinkage = None\n",
    "priors = [0.6697632062147972, 0.5557400774855933]\n",
    "\n",
    "# Normalizar priors para que sumen 1\n",
    "priors = np.array(priors)\n",
    "priors = priors / np.sum(priors)\n",
    "\n",
    "# Models:\n",
    "#lda = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage, priors=priors)\n",
    "lda = LinearDiscriminantAnalysis(solver=solver)\n",
    "\n",
    "# Train model:\n",
    "lda.fit(x_train_sfm, y_train)\n",
    "\n",
    "# Pred with train data:\n",
    "y_pred = lda.predict(x_train_sfm)\n",
    "\n",
    "# Pred with test data:\n",
    "y_pred2 = lda.predict(x_test_sfm)\n",
    "\n",
    "# Pred with shirt & sweater data:\n",
    "y_pred3 = lda.predict(x_testSS_sfm)\n",
    "\n",
    "# Calcular la precisión\n",
    "accuracy1 = accuracy_score(y_train, y_pred)\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "accuracy3 = accuracy_score(y_div2, y_pred3)\n",
    "\n",
    "print(\"Precisión del modelo en el conjunto de train: {:.2f}%\".format(accuracy1 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba: {:.2f}%\".format(accuracy2 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba de camisas y sweaters: {:.2f}%\".format(accuracy3 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92961712-f165-4f52-a2af-7234d17b9d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred2)\n",
    "\n",
    "# Graficar la matriz de confusión usando seaborn\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Verdaderos')\n",
    "plt.show()\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix2 = confusion_matrix(y_div2, y_pred3)\n",
    "\n",
    "# Graficar la matriz de confusión usando seaborn\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix2, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Verdaderos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3707c-2b80-4a07-9229-51cca1ac78d9",
   "metadata": {},
   "source": [
    "## KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5b137-438c-4c95-b3d7-9e3e9228b2cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    # Matriz de confusión: [TN, FP], [FN, TP]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    return accuracy, specificity, sensitivity\n",
    "\n",
    "def objective(trial):\n",
    "    # Sugerir el número de vecinos (sólo números impares)\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 1, 31, step=2)\n",
    "\n",
    "    # Sugerir la métrica de distancia\n",
    "    metric = trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'chebyshev', 'minkowski', 'hamming', 'canberra', 'seuclidean'])\n",
    "\n",
    "    # Configurar parámetros adicionales si se selecciona Minkowski o SEuclidean\n",
    "    if metric == 'minkowski':\n",
    "        p = trial.suggest_float('p', 1, 4)\n",
    "        metric_params = {'p': p}\n",
    "    elif metric == 'seuclidean':\n",
    "        # Calcular el vector de varianzas de los datos de entrenamiento para seuclidean\n",
    "        V = np.var(x_train_rfesvm, axis=0)\n",
    "        metric_params = {'V': V}\n",
    "    else:\n",
    "        metric_params = None\n",
    "\n",
    "    # Configurar validación cruzada\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    specificities = []\n",
    "    sensitivities = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(x_train_rfesvm, y_train):\n",
    "        x_train_fold, x_test_fold = x_train_rfesvm[train_idx], x_train_rfesvm[test_idx]\n",
    "        y_train_fold, y_test_fold = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "        # Crear el clasificador k-NN con los parámetros sugeridos\n",
    "        classifier = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, metric_params=metric_params)\n",
    "\n",
    "        # Entrenar el clasificador en el conjunto de entrenamiento\n",
    "        classifier.fit(x_train_fold, y_train_fold)\n",
    "\n",
    "        # Predecir el conjunto de prueba\n",
    "        y_pred = classifier.predict(x_test_fold)\n",
    "\n",
    "        # Evaluar el clasificador\n",
    "        accuracy, specificity, sensitivity = compute_metrics(y_test_fold, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        specificities.append(specificity)\n",
    "        sensitivities.append(sensitivity)\n",
    "\n",
    "    # Calcular métricas promedio y desviaciones estándar\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_specificity = np.mean(specificities)\n",
    "    avg_sensitivity = np.mean(sensitivities)\n",
    "\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    std_specificity = np.std(specificities)\n",
    "    std_sensitivity = np.std(sensitivities)\n",
    "\n",
    "    # Mostrar métricas\n",
    "    print(f\"Accuracy: {avg_accuracy:.3f} ± {std_accuracy:.3f} | \"\n",
    "          f\"Specificity: {avg_specificity:.3f} ± {std_specificity:.3f} | \"\n",
    "          f\"Sensitivity: {avg_sensitivity:.3f} ± {std_sensitivity:.3f}\")\n",
    "\n",
    "    # Retornar la precisión promedio\n",
    "    return avg_accuracy\n",
    "\n",
    "# Crear un estudio de Optuna y encontrar los mejores parámetros\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=400)\n",
    "\n",
    "# Imprimir los mejores parámetros\n",
    "print(\"Mejores parámetros: \", study.best_params)\n",
    "print(\"Mejor precisión obtenida: \", study.best_value)\n",
    "\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "# Ordenar los ensayos por su valor (mejor a peor) y tomar los primeros 10\n",
    "best_trials = sorted(study.trials, key=lambda trial: trial.value, reverse=True)[:10]\n",
    "\n",
    "print(\"\\nTop 10 Trials:\")\n",
    "for trial in best_trials:\n",
    "    print(f\"Trial {trial.number} | Value: {trial.value} | Params: {trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c414b41-34db-4bd1-9efa-495b37de9038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Parámetros obtenidos de Optuna\n",
    "V = np.var(x_train, axis=0)\n",
    "metric_params = {'V': V}\n",
    "\n",
    "# Models:\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric= 'canberra')\n",
    "\n",
    "# Train model:\n",
    "knn.fit(x_train_rfesvm, y_train)\n",
    "\n",
    "# Pred with train data:\n",
    "y_pred = knn.predict(x_train_rfesvm)\n",
    "\n",
    "# Pred with test data:\n",
    "y_pred2 = knn.predict(x_test_rfesvm)\n",
    "\n",
    "# Pred with shirt & sweater data:\n",
    "y_pred3 = knn.predict(x_testSS_rfesvm)\n",
    "\n",
    "# Calcular la precisión\n",
    "accuracy1 = accuracy_score(y_train, y_pred)\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "accuracy3 = accuracy_score(y_div2, y_pred3)\n",
    "\n",
    "print(\"Precisión del modelo en el conjunto de train: {:.2f}%\".format(accuracy1 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba: {:.2f}%\".format(accuracy2 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba de camisas y sweaters: {:.2f}%\".format(accuracy3 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a1272-e00b-4385-b238-198b9ae69b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred2)\n",
    "\n",
    "# Graficar la matriz de confusión usando seaborn\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Verdaderos')\n",
    "plt.show()\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix2 = confusion_matrix(y_div2, y_pred3)\n",
    "\n",
    "# Graficar la matriz de confusión usando seaborn\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix2, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Verdaderos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0fbd53-3bd3-4bfa-abcc-9463233425f0",
   "metadata": {},
   "source": [
    "## RF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c9845-4183-4c2f-9c7f-f65a3ccb874a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    # Matriz de confusión: [TN, FP], [FN, TP]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    return accuracy, specificity, sensitivity\n",
    "\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 50)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "\n",
    "    # Configurar validación cruzada\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    specificities = []\n",
    "    sensitivities = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(x_train_pca, y_train):\n",
    "        x_train_fold, x_test_fold = x_train_pca[train_idx], x_train_pca[test_idx]\n",
    "        y_train_fold, y_test_fold = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "        # Crear el clasificador RandomForest con los hiperparámetros sugeridos\n",
    "        classifier_obj = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        classifier_obj.fit(x_train_fold, y_train_fold)\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        y_pred = classifier_obj.predict(x_test_fold)\n",
    "        accuracy, specificity, sensitivity = compute_metrics(y_test_fold, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        specificities.append(specificity)\n",
    "        sensitivities.append(sensitivity)\n",
    "\n",
    "    # Calcular métricas promedio y desviaciones estándar\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_specificity = np.mean(specificities)\n",
    "    avg_sensitivity = np.mean(sensitivities)\n",
    "\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    std_specificity = np.std(specificities)\n",
    "    std_sensitivity = np.std(sensitivities)\n",
    "\n",
    "    # Mostrar métricas\n",
    "    print(f\"Accuracy: {avg_accuracy:.3f} ± {std_accuracy:.3f} | \"\n",
    "          f\"Specificity: {avg_specificity:.3f} ± {std_specificity:.3f} | \"\n",
    "          f\"Sensitivity: {avg_sensitivity:.3f} ± {std_sensitivity:.3f}\")\n",
    "\n",
    "    # Retornar la precisión promedio\n",
    "    return avg_accuracy\n",
    "\n",
    "# Crear y optimizar el estudio\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=150)\n",
    "\n",
    "print('Mejores parámetros:', study.best_params)\n",
    "print('Mejor resultado de precisión:', study.best_value)\n",
    "\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "# Ordenar los ensayos por su valor (mejor a peor) y tomar los primeros 10\n",
    "best_trials = sorted(study.trials, key=lambda trial: trial.value, reverse=True)[:10]\n",
    "\n",
    "print(\"\\nTop 10 Trials:\")\n",
    "for trial in best_trials:\n",
    "    print(f\"Trial {trial.number} | Value: {trial.value} | Params: {trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3a4f19-d28f-4587-b43a-a96e4fc6cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Models:\n",
    "rf = RandomForestClassifier(n_estimators=250, max_depth=6, min_samples_split=8, min_samples_leaf=6, max_features='log2', random_state=42)\n",
    "\n",
    "# Train model:\n",
    "rf.fit(x_train_pca, y_train)\n",
    "\n",
    "# Pred with train data:\n",
    "y_pred = rf.predict(x_train_pca)\n",
    "\n",
    "# Pred with test data:\n",
    "y_pred2 = rf.predict(x_test_pca)\n",
    "\n",
    "# Pred with shirt & sweater data:\n",
    "y_pred3 = rf.predict(x_testSS_pca)\n",
    "\n",
    "# Calcular la precisión\n",
    "accuracy1 = accuracy_score(y_train, y_pred)\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "accuracy3 = accuracy_score(y_div2, y_pred3)\n",
    "\n",
    "print(\"Precisión del modelo en el conjunto de train: {:.2f}%\".format(accuracy1 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba: {:.2f}%\".format(accuracy2 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba de camisas y sweaters: {:.2f}%\".format(accuracy3 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968afad-7983-4a92-8214-7d702b7fff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred2)\n",
    "\n",
    "# Graficar la matriz de confusión usando seaborn\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Verdaderos')\n",
    "plt.show()\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix2 = confusion_matrix(y_div2, y_pred3)\n",
    "\n",
    "# Graficar la matriz de confusión usando seaborn\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix2, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Verdaderos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14ea27-3bbc-4c8f-a806-caa9b15dd725",
   "metadata": {},
   "source": [
    "## MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ef0fa-930a-4c00-9c92-7a0128ce67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, InputLayer, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import numpy as np\n",
    "\n",
    "# MLP:\n",
    "\n",
    "# Modelo con 2 capas ocultas:\n",
    "\n",
    "def create_model(input_neurons, neurons_layer_1, neurons_layer_2, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_neurons))\n",
    "\n",
    "    # Primera capa oculta con Batch Normalization\n",
    "    model.add(Dense(neurons_layer_1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Segunda capa oculta\n",
    "    model.add(Dense(neurons_layer_2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "\n",
    "    # Capa de salida\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=1e-4)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e33161-8100-4c9e-834e-b7339df4b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_neurons = x_train_sfslda.shape[1]\n",
    "print(\"El valor de input_neurons es:\", input_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bf2d5e-f2b2-46bb-a396-4650239a9c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "best_model = None  # Variable global para almacenar el mejor modelo\n",
    "best_accuracy = 0\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    # Matriz de confusión: [TN, FP], [FN, TP]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    return accuracy, specificity, sensitivity\n",
    "\n",
    "def objective(trial):\n",
    "    global best_model, best_accuracy\n",
    "\n",
    "    # Sugerir una semilla para la inicialización de los pesos\n",
    "    seed = trial.suggest_int('seed', 1, 10000)\n",
    "\n",
    "    # Establecer la semilla en TensorFlow y NumPy\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    input_neurons = x_train_sfslda.shape[1]\n",
    "    neurons_layer_1 = trial.suggest_categorical('neurons_layer_1', [64, 128, 200])\n",
    "    neurons_layer_2 = trial.suggest_categorical('neurons_layer_2', [16, 32, 64])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.6)\n",
    "    \n",
    "    # Configurar validación cruzada\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    accuracies = []\n",
    "    specificities = []\n",
    "    sensitivities = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(x_train_sfslda, y_train):\n",
    "        x_train_fold, x_test_fold = x_train_sfslda[train_idx], x_train_sfslda[test_idx]\n",
    "        y_train_fold, y_test_fold = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "        # Crear el modelo usando la función que ya tienes\n",
    "        model = create_model(input_neurons, neurons_layer_1, neurons_layer_2, dropout_rate)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=150, restore_best_weights=True)\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        model.fit(x_train_fold, y_train_fold, epochs=400, verbose=0, batch_size=32, \n",
    "                  validation_data=(x_test_fold, y_test_fold), callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        y_pred = (model.predict(x_test_fold) > 0.5).astype(int)\n",
    "        accuracy, specificity, sensitivity = compute_metrics(y_test_fold, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        specificities.append(specificity)\n",
    "        sensitivities.append(sensitivity)\n",
    "\n",
    "    # Calcular métricas promedio y desviaciones estándar\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_specificity = np.mean(specificities)\n",
    "    avg_sensitivity = np.mean(sensitivities)\n",
    "\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    std_specificity = np.std(specificities)\n",
    "    std_sensitivity = np.std(sensitivities)\n",
    "\n",
    "    # Si este modelo es mejor, actualizamos la referencia del mejor modelo\n",
    "    if avg_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_accuracy\n",
    "        best_model = model  # Sustituye el modelo anterior\n",
    "\n",
    "    # Mostrar métricas\n",
    "    print(f\"Seed: {seed} | Accuracy: {avg_accuracy:.3f} ± {std_accuracy:.3f} | \"\n",
    "          f\"Specificity: {avg_specificity:.3f} ± {std_specificity:.3f} | \"\n",
    "          f\"Sensitivity: {avg_sensitivity:.3f} ± {std_sensitivity:.3f}\")\n",
    "\n",
    "    # Retornar la precisión promedio\n",
    "    return avg_accuracy\n",
    "\n",
    "# Resto del código para ejecutar el estudio de Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print('Mejores parámetros:', study.best_params)\n",
    "print('Mejor resultado de precisión:', study.best_value)\n",
    "\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "# Ordenar los ensayos por su valor (mejor a peor) y tomar los primeros 10\n",
    "best_trials = sorted(study.trials, key=lambda trial: trial.value, reverse=True)[:10]\n",
    "\n",
    "print(\"\\nTop 10 Trials:\")\n",
    "for trial in best_trials:\n",
    "    print(f\"Trial {trial.number} | Value: {trial.value} | Params: {trial.params} | Seed: {trial.params['seed']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1be51a-e079-42a0-89dd-6c94eb6c8cbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Supón que la semilla que obtuviste es 1234\n",
    "seed = 708\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "input_neurons = x_train_sfslda.shape[1]\n",
    "\n",
    "model = create_model(input_neurons, 64, 64, 0.3948373603618849)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=150, restore_best_weights=True)\n",
    "\n",
    "model.fit(x_train_sfslda, y_train, epochs=400, verbose=1, batch_size=32, validation_data=(x_testSS_sfslda, y_div2), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d043304-3711-437c-9ceb-69201c8ea502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pred with train data:\n",
    "y_pred = (model.predict(x_train_sfslda) > 0.5).astype(int)\n",
    "\n",
    "# Pred with test data:\n",
    "y_pred2 = (model.predict(x_test_sfslda) > 0.5).astype(int)\n",
    "\n",
    "# Pred with shirt & sweater data:\n",
    "y_pred3 = (model.predict(x_testSS_sfslda) > 0.5).astype(int)\n",
    "\n",
    "# Calcular la precisión\n",
    "accuracy1 = accuracy_score(y_train, y_pred)\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "accuracy3 = accuracy_score(y_div2, y_pred3)\n",
    "\n",
    "print(\"Precisión del modelo en el conjunto de train: {:.2f}%\".format(accuracy1 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba: {:.2f}%\".format(accuracy2 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba de camisas y sweaters: {:.2f}%\".format(accuracy3 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdf25f-a825-48da-bb3e-8fd22b84b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred2)\n",
    "\n",
    "# Graficar la matriz de confusión usando seaborn\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Verdaderos')\n",
    "plt.show()\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix2 = confusion_matrix(y_div2, y_pred3)\n",
    "\n",
    "# Graficar la matriz de confusión usando seaborn\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix2, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Verdaderos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51339940-7ec1-4ca1-9759-5bf90ae693ad",
   "metadata": {},
   "source": [
    "# Saving models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a185c-a62c-4c3a-8190-913ab2af0982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Guardar el modelo\n",
    "dump(svm, 'svm.joblib')\n",
    "dump(lda, 'lda.joblib')\n",
    "dump(knn, 'knn.joblib')\n",
    "dump(rf, 'rf.joblib')\n",
    "\n",
    "# Guardar el modelo en el disco\n",
    "model_path = '/home/andres_marin/Notebooks/Tesis/Finals_tests/All bands - all channels/Models/mlp.h5'  # La extensión .h5 indica que se usa el formato HDF5\n",
    "model.save(model_path)\n",
    "print(f\"Modelo 1 guardado en {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08dbbd-5bff-451e-b549-44afced757a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo en el disco\n",
    "model_path = '/home/andres_marin/Notebooks/Tesis/Finals_tests/All bands - all channels/mlp2.h5'  # La extensión .h5 indica que se usa el formato HDF5\n",
    "model.save(model_path)\n",
    "print(f\"Modelo 1 guardado en {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74bfbab-82dd-42ad-8e47-807af645da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "dump(rf, 'rf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b0285-aa6e-4592-8996-e7dc863f5ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
