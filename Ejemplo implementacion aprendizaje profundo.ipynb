{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d05d321-b448-4876-86eb-3d76484daf18",
   "metadata": {},
   "source": [
    "# Load the EEG Signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a26c42-f08c-411d-bc25-c7c3dcabbb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "# Carga el archivo .mat\n",
    "mat = scipy.io.loadmat('eegdata.mat')\n",
    "mat2= scipy.io.loadmat('eeg_shirt_sweat.mat')\n",
    "\n",
    "# Extrae el contenido bajo la clave 'eeg_Data'\n",
    "eeg_signals = mat['eeg_Data']\n",
    "eeg_signals2 = mat2['eeg_Data']\n",
    "\n",
    "# Iterar sobre las celdas para extraer cada señal\n",
    "signals = [eeg_signals[0, i] for i in range(eeg_signals.shape[1])]\n",
    "signals_ss = [eeg_signals[0,i] for i in range(eeg_signals2.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e484a0-0518-4f26-a5bc-b523d016c0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(895, 8, 512)\n",
      "(150, 8, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convertir la lista de señales en una matriz 3D\n",
    "eeg = np.stack(signals).transpose(0, 2, 1)\n",
    "eeg_ss=np.stack(signals_ss).transpose(0,2,1)\n",
    "\n",
    "# Índices de los canales frontales\n",
    "frontal_indices = [0, 1, 2, 3, 10, 11, 12, 13]\n",
    "\n",
    "# Filtrar solo los canales frontales\n",
    "eeg = eeg[:, frontal_indices, :]\n",
    "eeg_ss= eeg_ss[:, frontal_indices, :]\n",
    "\n",
    "# Mostrar la forma de las matrices con solo canales frontales\n",
    "print(eeg.shape)\n",
    "print(eeg_ss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec20951c-6676-4d3c-bb86-5cd0721c8d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Definir el filtro pasa-banda\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data, axis=0)  # Aplicar el filtro en el eje de las muestras\n",
    "    return y\n",
    "\n",
    "# Parámetros del filtro\n",
    "lowcut = 1.0  # Frecuencia de corte baja\n",
    "highcut = 40.0  # Frecuencia de corte alta\n",
    "fs = 128  # Frecuencia de muestreo en Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6d3291-9f65-4aea-98c3-c9d6c4920381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(895, 8, 512)\n",
      "(150, 8, 512)\n"
     ]
    }
   ],
   "source": [
    "# Aplicar el filtro pasa-banda a cada canal de cada señal\n",
    "filtered_data = np.zeros_like(eeg)\n",
    "\n",
    "num_signals, num_channels, num_samples = eeg.shape\n",
    "\n",
    "# Aplicar el filtro pasa-banda a cada canal de cada señal\n",
    "filtered_data = np.zeros_like(eeg)\n",
    "\n",
    "for i in range(num_signals):\n",
    "    for j in range(num_channels):\n",
    "        filtered_data[i, j, :] = bandpass_filter(eeg[i, j, :], lowcut, highcut, fs)\n",
    "\n",
    "# Aplicar el filtro pasa-banda a cada canal de cada señal\n",
    "filtered_data2 = np.zeros_like(eeg_ss)\n",
    "\n",
    "num_signals, num_channels, num_samples = eeg_ss.shape\n",
    "\n",
    "for i in range(num_signals):\n",
    "    for j in range(num_channels):\n",
    "        filtered_data2[i, j, :] = bandpass_filter(eeg_ss[i, j, :], lowcut, highcut, fs)\n",
    "\n",
    "# Mostrar la forma de la matriz 3D\n",
    "print(filtered_data.shape)\n",
    "print(filtered_data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0612b30f-78a1-43e9-927f-d92861d0f120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape: (895,)\n",
      "Firts values: [1 0 1 0 1 0 0 1 0 1]\n",
      "Labels dsitribution: Counter({0: 502, 1: 393})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Shirt and sweater data:\n",
    "dftrainSS = pd.read_csv(\"/home/andres_marin/Notebooks/Tesis/labels/Labels_Shirt_Sweater.csv\",\n",
    "                     header=None)\n",
    "labelsSS = dftrainSS.iloc[:, 0].values\n",
    "\n",
    "df = pd.read_csv(\"/home/andres_marin/Notebooks/Tesis/labels/Labels_All_Data.csv\",\n",
    "                     header=None)\n",
    "labels = df.iloc[:, 0].values\n",
    "\n",
    "# Verificar la forma y la distribución de las etiquetas\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"Firts values:\", labels[:10])  # Muestra las primeras 10 etiquetas\n",
    "\n",
    "# Verificar la distribución de las etiquetas\n",
    "label_distribution = Counter(labels)\n",
    "print(\"Labels dsitribution:\", label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd819fff-79c9-459c-aed2-81d46f624c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "x_div1, x_div2, y_div1, y_div2 = train_test_split(\n",
    "    filtered_data2, labelsSS, test_size=0.50, stratify=labelsSS, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e18d836-11a0-40e4-a5e8-5afc27116687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribución de clases en el conjunto de entrenamiento: Counter({0: 376, 1: 295})\n",
      "Distribución de clases en el conjunto de validación: Counter({0: 126, 1: 98})\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    filtered_data, labels, test_size=0.25, stratify=labels, random_state=42)\n",
    "\n",
    "# Verificar la distribución de las clases en los conjuntos de entrenamiento y validación\n",
    "train_counts = Counter(y_train)\n",
    "val_counts = Counter(y_test)\n",
    "\n",
    "print(\"\\nDistribución de clases en el conjunto de entrenamiento:\", train_counts)\n",
    "print(\"Distribución de clases en el conjunto de validación:\", val_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4151358c-f389-43d3-bc2c-61867476f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que tus etiquetas están en un vector con valores 0 o 1\n",
    "y_train_cat = to_categorical(y_train, num_classes=2)\n",
    "y_test1_cat = to_categorical(y_test1, num_classes=2)\n",
    "y_test_cat = to_categorical(y_test, num_classes=2)\n",
    "y_div2_cat = to_categorical(y_div2, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27312b8-4368-4d29-a653-38abca0f52d4",
   "metadata": {},
   "source": [
    "# EEGNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a81dac2-6325-404f-aa66-317d4fcb906c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import tensorflow as tf\n",
    "from EEGModels import EEGNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# Lista para almacenar los mejores modelos y sus respectivas precisiones\n",
    "top_models = []\n",
    "top_accuracies = []\n",
    "\n",
    "# Lista para almacenar los mejores modelos y sus métricas\n",
    "top_models = []\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    # Confusion matrix: [TN, FP], [FN, TP]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    return accuracy, specificity, sensitivity\n",
    "\n",
    "def objective(trial):\n",
    "    # Hiperparámetros que deseas optimizar\n",
    "    seed = trial.suggest_int('seed', 0, 1000000)\n",
    "\n",
    "    # Configurar la semilla\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # Configurar la validación cruzada\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    \n",
    "    accuracies = []\n",
    "    specificities = []\n",
    "    sensitivities = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(x_data, y_data):\n",
    "        x_train, x_test = x_data[train_idx], x_data[test_idx]\n",
    "        y_train, y_test = y_data[train_idx], y_data[test_idx]\n",
    "\n",
    "        # Convertir etiquetas a categóricas\n",
    "        y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "        y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "        # Crear el modelo\n",
    "        model = EEGNet(nb_classes=2, Chans=num_channels, Samples=num_samples)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        model.fit(x_train, y_train_cat, epochs=300, batch_size=32, verbose=0, \n",
    "                  validation_data=(x_test, y_test_cat), callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "        accuracy, specificity, sensitivity = compute_metrics(y_test, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        specificities.append(specificity)\n",
    "        sensitivities.append(sensitivity)\n",
    "\n",
    "    # Calcular métricas promedio y desviaciones estándar\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_specificity = np.mean(specificities)\n",
    "    avg_sensitivity = np.mean(sensitivities)\n",
    "\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    std_specificity = np.std(specificities)\n",
    "    std_sensitivity = np.std(sensitivities)\n",
    "\n",
    "    # Guardar el modelo si está en el top 5 basado en la métrica promedio de exactitud\n",
    "    top_models.append((model, avg_accuracy, avg_specificity, avg_sensitivity))\n",
    "    top_models.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if len(top_models) > 5:\n",
    "        top_models.pop()\n",
    "\n",
    "    # Mostrar métricas por cada evaluación\n",
    "    print(f\"Seed: {seed} | Accuracy: {avg_accuracy:.3f} ± {std_accuracy:.3f} | \"\n",
    "          f\"Specificity: {avg_specificity:.3f} ± {std_specificity:.3f} | \"\n",
    "          f\"Sensitivity: {avg_sensitivity:.3f} ± {std_sensitivity:.3f}\")\n",
    "\n",
    "    return avg_accuracy\n",
    "\n",
    "# Crear el estudio de Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Obtener la mejor semilla\n",
    "best_seed = study.best_trial.params['seed']\n",
    "print(f\"La mejor semilla es: {best_seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ad129-d540-4373-b495-5135670664d8",
   "metadata": {},
   "source": [
    "**Tests:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b151145-ca4e-4e09-ac43-bb20369721bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model:\n",
    "best_model = top_models[0][0]\n",
    "\n",
    "# Pred with train data:\n",
    "y_pred = best_model.predict(x_train)\n",
    "\n",
    "# Pred with test data:\n",
    "y_pred2 = best_model.predict(x_test1)\n",
    "\n",
    "y_pred3 = best_model.predict(x_div2)\n",
    "\n",
    "# Pred with shirt & sweater data:\n",
    "#y_pred3 = (model.predict(x_testss_nuevo) > 0.5).astype(int)\n",
    "\n",
    "# Calcular la precisión\n",
    "accuracy1 = accuracy_score(np.argmax(y_train_cat, axis=1), np.argmax(y_pred, axis=1))\n",
    "accuracy2 = accuracy_score(np.argmax(y_test1_cat, axis=1), np.argmax(y_pred2, axis=1))\n",
    "accuracy3 = accuracy_score(np.argmax(y_div2_cat, axis=1), np.argmax(y_pred3, axis=1))\n",
    "#accuracy3 = accuracy_score(y_div2, y_pred3)\n",
    "\n",
    "print(\"Precisión del modelo en el conjunto de train: {:.2f}%\".format(accuracy1 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba: {:.2f}%\".format(accuracy2 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba: {:.2f}%\".format(accuracy3 * 100))\n",
    "#print(\"Precisión del modelo en el conjunto de prueba de camisas y sweaters: {:.2f}%\".format(accuracy3 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3978df4-367f-4226-87d2-d1040b13f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo en el disco\n",
    "model_path = '/home/andres_marin/Notebooks/Tesis/Deep_Learning_Test/Models/Frontal_ch_EEGNet_75_70_2.h5'  # La extensión .h5 indica que se usa el formato HDF5\n",
    "best_model.save(model_path)\n",
    "print(f\"Modelo 1 guardado en {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c2961f-31ae-4fa0-9857-e149ee3ae00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred2 = (y_pred2 > 0.5).astype(int)\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred2)\n",
    "\n",
    "# Graficar la matriz de confusión usando seaborn\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Verdaderos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ca267-00d6-4cb8-b4ea-4185c5246c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d77fed97-e98b-4d58-a90d-8e340dda91ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy percentage: 68.88888888888889\n",
      "specificity_percentage: 68.42105263157895\n",
      "recall_percentage: 69.23076923076923\n"
     ]
    }
   ],
   "source": [
    "# Valores de la nueva matriz de confusión\n",
    "TP = 90\n",
    "FN = 40\n",
    "FP = 30\n",
    "TN = 65\n",
    "\n",
    "\n",
    "# Cálculo de métricas en porcentaje\n",
    "accuracy_percentage = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "precision_percentage = (TP / (TP + FP)) * 100\n",
    "specificity_percentage = (TN / (TN + FP)) * 100\n",
    "recall_percentage = (TP / (TP + FN)) * 100\n",
    "f1_score_percentage = 2 * (precision_percentage * recall_percentage) / (precision_percentage + recall_percentage)\n",
    "\n",
    "accuracy_percentage, precision_percentage, specificity_percentage, recall_percentage, f1_score_percentage\n",
    "\n",
    "#mcc = calculate_mcc(TP, TN, FP, FN)* 100\n",
    "\n",
    "balanced_accuracy = (recall_percentage + specificity_percentage)/2\n",
    "\n",
    "print(\"Accuracy percentage:\", accuracy_percentage)\n",
    "#print(\"precision_percentage:\", precision_percentage)\n",
    "print(\"specificity_percentage:\", specificity_percentage)\n",
    "print(\"recall_percentage:\", recall_percentage)\n",
    "#print(\"f1_score_percentage:\", f1_score_percentage)\n",
    "#print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
    "#print(\"Balanced accuracy:\", balanced_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6f647-edd5-4780-9e46-152204a556d6",
   "metadata": {},
   "source": [
    "# DeepConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d144e3-1d32-4e96-9c9a-0108018f96c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import tensorflow as tf\n",
    "from EEGModels import EEGNet, DeepConvNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# Lista para almacenar los mejores modelos y sus respectivas precisiones\n",
    "top_models = []\n",
    "top_accuracies = []\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    # Confusion matrix: [TN, FP], [FN, TP]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    return accuracy, specificity, sensitivity\n",
    "\n",
    "def objective(trial):\n",
    "    # Hiperparámetros que deseas optimizar\n",
    "    seed = trial.suggest_int('seed', 0, 1000000)\n",
    "\n",
    "    # Configurar la semilla\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # Configurar la validación cruzada\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    \n",
    "    accuracies = []\n",
    "    specificities = []\n",
    "    sensitivities = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(x_data, y_data):\n",
    "        x_train, x_test = x_data[train_idx], x_data[test_idx]\n",
    "        y_train, y_test = y_data[train_idx], y_data[test_idx]\n",
    "\n",
    "        # Convertir etiquetas a categóricas\n",
    "        y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "        y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "        # Crear el modelo\n",
    "        model = DeepConvNet(nb_classes=2, Chans=num_channels, Samples=num_samples)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        model.fit(x_train, y_train_cat, epochs=300, batch_size=32, verbose=0, \n",
    "                  validation_data=(x_test, y_test_cat), callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "        accuracy, specificity, sensitivity = compute_metrics(y_test, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        specificities.append(specificity)\n",
    "        sensitivities.append(sensitivity)\n",
    "\n",
    "    # Calcular métricas promedio y desviaciones estándar\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_specificity = np.mean(specificities)\n",
    "    avg_sensitivity = np.mean(sensitivities)\n",
    "\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    std_specificity = np.std(specificities)\n",
    "    std_sensitivity = np.std(sensitivities)\n",
    "\n",
    "    # Guardar el modelo si está en el top 5 basado en la métrica promedio de exactitud\n",
    "    top_models.append((model, avg_accuracy, avg_specificity, avg_sensitivity))\n",
    "    top_models.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if len(top_models) > 5:\n",
    "        top_models.pop()\n",
    "\n",
    "    # Mostrar métricas por cada evaluación\n",
    "    print(f\"Seed: {seed} | Accuracy: {avg_accuracy:.3f} ± {std_accuracy:.3f} | \"\n",
    "          f\"Specificity: {avg_specificity:.3f} ± {std_specificity:.3f} | \"\n",
    "          f\"Sensitivity: {avg_sensitivity:.3f} ± {std_sensitivity:.3f}\")\n",
    "\n",
    "    return avg_accuracy\n",
    "\n",
    "# Crear el estudio de Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Obtener la mejor semilla\n",
    "best_seed = study.best_trial.params['seed']\n",
    "print(f\"La mejor semilla es: {best_seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d75a3d-0858-4fda-8903-ebcb12b250be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model:\n",
    "best_model = top_models[3][0]\n",
    "\n",
    "# Pred with train data:\n",
    "y_pred = best_model.predict(x_train)\n",
    "\n",
    "# Pred with test data:\n",
    "y_pred2 = best_model.predict(x_test1)\n",
    "\n",
    "y_pred3 = best_model.predict(x_div2)\n",
    "\n",
    "# Pred with shirt & sweater data:\n",
    "#y_pred3 = (model.predict(x_testss_nuevo) > 0.5).astype(int)\n",
    "\n",
    "# Calcular la precisión\n",
    "accuracy1 = accuracy_score(np.argmax(y_train_cat, axis=1), np.argmax(y_pred, axis=1))\n",
    "accuracy2 = accuracy_score(np.argmax(y_test1_cat, axis=1), np.argmax(y_pred2, axis=1))\n",
    "accuracy3 = accuracy_score(np.argmax(y_div2_cat, axis=1), np.argmax(y_pred3, axis=1))\n",
    "#accuracy3 = accuracy_score(y_div2, y_pred3)\n",
    "\n",
    "print(\"Precisión del modelo en el conjunto de train: {:.2f}%\".format(accuracy1 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba: {:.2f}%\".format(accuracy2 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba: {:.2f}%\".format(accuracy3 * 100))\n",
    "#print(\"Precisión del modelo en el conjunto de prueba de camisas y sweaters: {:.2f}%\".format(accuracy3 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ab3a8-54bf-4952-bc63-d07cd4551b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo en el disco\n",
    "model_path = '/home/andres_marin/Notebooks/Tesis/Deep_Learning_Test/Models/DeepNet_77_73.h5'  # La extensión .h5 indica que se usa el formato HDF5\n",
    "best_model.save(model_path)\n",
    "print(f\"Modelo 1 guardado en {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b74f69-11a2-46cb-a38f-5b04066bfb2f",
   "metadata": {},
   "source": [
    "# ShallowConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad80efd-039e-4280-9d65-a40d706e7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import tensorflow as tf\n",
    "from EEGModels import EEGNet, ShallowConvNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# Lista para almacenar los mejores modelos y sus respectivas precisiones\n",
    "top_models = []\n",
    "top_accuracies = []\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    # Confusion matrix: [TN, FP], [FN, TP]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    return accuracy, specificity, sensitivity\n",
    "\n",
    "def objective(trial):\n",
    "    # Hiperparámetros que deseas optimizar\n",
    "    seed = trial.suggest_int('seed', 0, 1000000)\n",
    "\n",
    "    # Configurar la semilla\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # Configurar la validación cruzada\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    \n",
    "    accuracies = []\n",
    "    specificities = []\n",
    "    sensitivities = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(x_data, y_data):\n",
    "        x_train, x_test = x_data[train_idx], x_data[test_idx]\n",
    "        y_train, y_test = y_data[train_idx], y_data[test_idx]\n",
    "\n",
    "        # Convertir etiquetas a categóricas\n",
    "        y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "        y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "        # Crear el modelo\n",
    "        model = ShallowConvNet(nb_classes=2, Chans=num_channels, Samples=num_samples)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        model.fit(x_train, y_train_cat, epochs=300, batch_size=32, verbose=0, \n",
    "                  validation_data=(x_test, y_test_cat), callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "        accuracy, specificity, sensitivity = compute_metrics(y_test, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        specificities.append(specificity)\n",
    "        sensitivities.append(sensitivity)\n",
    "\n",
    "    # Calcular métricas promedio y desviaciones estándar\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_specificity = np.mean(specificities)\n",
    "    avg_sensitivity = np.mean(sensitivities)\n",
    "\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    std_specificity = np.std(specificities)\n",
    "    std_sensitivity = np.std(sensitivities)\n",
    "\n",
    "    # Guardar el modelo si está en el top 5 basado en la métrica promedio de exactitud\n",
    "    top_models.append((model, avg_accuracy, avg_specificity, avg_sensitivity))\n",
    "    top_models.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if len(top_models) > 5:\n",
    "        top_models.pop()\n",
    "\n",
    "    # Mostrar métricas por cada evaluación\n",
    "    print(f\"Seed: {seed} | Accuracy: {avg_accuracy:.3f} ± {std_accuracy:.3f} | \"\n",
    "          f\"Specificity: {avg_specificity:.3f} ± {std_specificity:.3f} | \"\n",
    "          f\"Sensitivity: {avg_sensitivity:.3f} ± {std_sensitivity:.3f}\")\n",
    "\n",
    "    return avg_accuracy\n",
    "\n",
    "# Crear el estudio de Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Obtener la mejor semilla\n",
    "best_seed = study.best_trial.params['seed']\n",
    "print(f\"La mejor semilla es: {best_seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a36b80-55bc-478d-b816-99724ea3b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model:\n",
    "best_model = top_models[2][0]\n",
    "\n",
    "# Pred with train data:\n",
    "y_pred = best_model.predict(x_train)\n",
    "\n",
    "# Pred with test data:\n",
    "y_pred2 = best_model.predict(x_test1)\n",
    "\n",
    "y_pred3 = best_model.predict(x_div2)\n",
    "\n",
    "# Pred with shirt & sweater data:\n",
    "#y_pred3 = (model.predict(x_testss_nuevo) > 0.5).astype(int)\n",
    "\n",
    "# Calcular la precisión\n",
    "accuracy1 = accuracy_score(np.argmax(y_train_cat, axis=1), np.argmax(y_pred, axis=1))\n",
    "accuracy2 = accuracy_score(np.argmax(y_test1_cat, axis=1), np.argmax(y_pred2, axis=1))\n",
    "accuracy3 = accuracy_score(np.argmax(y_div2_cat, axis=1), np.argmax(y_pred3, axis=1))\n",
    "#accuracy3 = accuracy_score(y_div2, y_pred3)\n",
    "\n",
    "print(\"Precisión del modelo en el conjunto de train: {:.2f}%\".format(accuracy1 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba: {:.2f}%\".format(accuracy2 * 100))\n",
    "print(\"Precisión del modelo en el conjunto de prueba: {:.2f}%\".format(accuracy3 * 100))\n",
    "#print(\"Precisión del modelo en el conjunto de prueba de camisas y sweaters: {:.2f}%\".format(accuracy3 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca07867-fa53-49a7-bdec-cc3639d14fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo en el disco\n",
    "model_path = '/home/andres_marin/Notebooks/Tesis/Deep_Learning_Test/Models/ShallowConvNet.h5'  # La extensión .h5 indica que se usa el formato HDF5\n",
    "best_model.save(model_path)\n",
    "print(f\"Modelo 1 guardado en {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41798f0d-361d-459b-9de9-6393b2075aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
